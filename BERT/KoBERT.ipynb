{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":431},"executionInfo":{"elapsed":18900,"status":"ok","timestamp":1724048251699,"user":{"displayName":"­최은빈(자연과학대학 통계학과)","userId":"17481316940076121545"},"user_tz":-540},"id":"oq27CeMKPJSQ","outputId":"a105edce-930e-4290-be3e-be67070b551b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting numpy==1.23.1\n","  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n","Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: numpy\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.26.4\n","    Uninstalling numpy-1.26.4:\n","      Successfully uninstalled numpy-1.26.4\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xgboost 2.1.1 requires nvidia-nccl-cu12; platform_system == \"Linux\" and platform_machine != \"aarch64\", which is not installed.\n","albucore 0.0.13 requires numpy\u003c2,\u003e=1.24.4, but you have numpy 1.23.1 which is incompatible.\n","albumentations 1.4.13 requires numpy\u003e=1.24.4, but you have numpy 1.23.1 which is incompatible.\n","chex 0.1.86 requires numpy\u003e=1.24.1, but you have numpy 1.23.1 which is incompatible.\n","pandas-stubs 2.1.4.231227 requires numpy\u003e=1.26.0; python_version \u003c \"3.13\", but you have numpy 1.23.1 which is incompatible.\n","tensorflow 2.17.0 requires numpy\u003c2.0.0,\u003e=1.23.5; python_version \u003c= \"3.11\", but you have numpy 1.23.1 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed numpy-1.23.1\n"]},{"data":{"application/vnd.colab-display-data+json":{"id":"beeed71a6225495aa60ae85e72044a3e","pip_warning":{"packages":["numpy"]}}},"metadata":{},"output_type":"display_data"}],"source":["# 실행 후 다시 실행\n","!pip install numpy==1.23.1"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40303,"status":"ok","timestamp":1724048300107,"user":{"displayName":"­최은빈(자연과학대학 통계학과)","userId":"17481316940076121545"},"user_tz":-540},"id":"TcXiLbSmO8RB","outputId":"79d16bac-0e24-4104-a0dc-1a2c9f84533c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting mxnet\n","  Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n","Requirement already satisfied: numpy\u003c2.0.0,\u003e1.16.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (1.23.1)\n","Requirement already satisfied: requests\u003c3,\u003e=2.20.0 in /usr/local/lib/python3.10/dist-packages (from mxnet) (2.32.3)\n","Collecting graphviz\u003c0.9.0,\u003e=0.8.1 (from mxnet)\n","  Downloading graphviz-0.8.4-py2.py3-none-any.whl.metadata (6.4 kB)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests\u003c3,\u003e=2.20.0-\u003emxnet) (2024.7.4)\n","Downloading mxnet-1.9.1-py3-none-manylinux2014_x86_64.whl (49.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n","Installing collected packages: graphviz, mxnet\n","  Attempting uninstall: graphviz\n","    Found existing installation: graphviz 0.20.3\n","    Uninstalling graphviz-0.20.3:\n","      Successfully uninstalled graphviz-0.20.3\n","Successfully installed graphviz-0.8.4 mxnet-1.9.1\n","Collecting gluonnlp\n","  Downloading gluonnlp-0.10.0.tar.gz (344 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.5/344.5 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy\u003e=1.16.0 in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (1.23.1)\n","Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (3.0.11)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gluonnlp) (24.1)\n","Building wheels for collected packages: gluonnlp\n","  Building wheel for gluonnlp (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for gluonnlp: filename=gluonnlp-0.10.0-cp310-cp310-linux_x86_64.whl size=661654 sha256=c68e97891af866996fe98c213d4306a81ba6e239387d1cfebc1112595c3dd96b\n","  Stored in directory: /root/.cache/pip/wheels/1a/1e/0d/99f55911d90f2b95b9f7c176d5813ef3622894a4b30fde6bd3\n","Successfully built gluonnlp\n","Installing collected packages: gluonnlp\n","Successfully installed gluonnlp-0.10.0\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n","Requirement already satisfied: huggingface-hub\u003c1.0,\u003e=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n","Requirement already satisfied: numpy\u003c2.0,\u003e=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.1)\n","Requirement already satisfied: packaging\u003e=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n","Requirement already satisfied: pyyaml\u003e=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: safetensors\u003e=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n","Requirement already satisfied: tokenizers\u003c0.20,\u003e=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n","Requirement already satisfied: fsspec\u003e=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.23.2-\u003etransformers) (2024.6.1)\n","Requirement already satisfied: typing-extensions\u003e=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub\u003c1.0,\u003e=0.23.2-\u003etransformers) (4.12.2)\n","Requirement already satisfied: charset-normalizer\u003c4,\u003e=2 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.3.2)\n","Requirement already satisfied: idna\u003c4,\u003e=2.5 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (3.7)\n","Requirement already satisfied: urllib3\u003c3,\u003e=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2.0.7)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests-\u003etransformers) (2024.7.4)\n"]}],"source":["!pip install mxnet\n","!pip install gluonnlp\n","!pip install sentencepiece\n","!pip install transformers"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8637,"status":"ok","timestamp":1724048308723,"user":{"displayName":"­최은빈(자연과학대학 통계학과)","userId":"17481316940076121545"},"user_tz":-540},"id":"7v5epvStPBxl","outputId":"43164945-5d68-4849-9024-56e791d85ce5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting kobert_tokenizer\n","  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-ezm759iv/kobert-tokenizer_c591875f476b4c22856a65e49b81d4ed\n","  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-ezm759iv/kobert-tokenizer_c591875f476b4c22856a65e49b81d4ed\n","  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Building wheels for collected packages: kobert_tokenizer\n","  Building wheel for kobert_tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert_tokenizer: filename=kobert_tokenizer-0.1-py3-none-any.whl size=4632 sha256=85380891eca972c7648791bc29f6a2006e7300fe5358ef263c416c8a71f2e8a9\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-uqdwc06i/wheels/e9/1a/3f/a864970e8a169c176befa3c4a1e07aa612f69195907a4045fe\n","Successfully built kobert_tokenizer\n","Installing collected packages: kobert_tokenizer\n","Successfully installed kobert_tokenizer-0.1\n"]}],"source":["!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer\u0026subdirectory=kobert_hf'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDKLbQSTPDc7"},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","import gluonnlp as nlp\n","import numpy as np\n","from tqdm import tqdm, tqdm_notebook"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HxbA3IryPDbQ"},"outputs":[],"source":["# Hugging Face를 통한 모델 및 토크나이저 Import\n","from kobert_tokenizer import KoBERTTokenizer\n","from transformers import BertModel\n","\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YO96MJMQPDZT"},"outputs":[],"source":["device = torch.device(\"cuda:0\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VHc1A_XuPDXR"},"outputs":[],"source":["tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n","bertmodel = BertModel.from_pretrained('skt/kobert-base-v1', return_dict=False)\n","vocab = nlp.vocab.BERTVocab.from_sentencepiece(tokenizer.vocab_file, padding_token='[PAD]')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Lh4HTQMXPDS4"},"outputs":[{"ename":"ValueError","evalue":"mount failed","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-7-c790245522a3\u003e\u001b[0m in \u001b[0;36m\u003ccell line: 3\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#구글 드라이브 import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         )\n\u001b[0;32m--\u003e 283\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: mount failed"]}],"source":["#구글 드라이브 import\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S9fpFgdSQj-D"},"outputs":[],"source":["import pandas as pd\n","df = pd.read_csv(\"/content/drive/MyDrive/유런 24 여름 방학 프로젝트/eda\u0026전처리/preprocessing_final.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uZuCSIH3Ri88"},"outputs":[],"source":["#feature label 나누기\n","data_list = []\n","for title, label in zip(df['processed_title'], df['label']):\n","  data = []\n","  data.append(title)\n","  data.append(str(label))\n","\n","  data_list.append(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Al7gaBH2QGgD"},"outputs":[],"source":["class BERTDataset(Dataset):\n","    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer,vocab, max_len,\n","                 pad, pair):\n","\n","        transform = nlp.data.BERTSentenceTransform(\n","            bert_tokenizer, max_seq_length=max_len,vocab=vocab, pad=pad, pair=pair)\n","\n","        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n","        self.labels = [np.int32(i[label_idx]) for i in dataset]\n","\n","    def __getitem__(self, i):\n","        return (self.sentences[i] + (self.labels[i], ))\n","\n","\n","    def __len__(self):\n","        return (len(self.labels))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yqPQsl0QGbw"},"outputs":[],"source":["# Setting parameters\n","max_len = 128\n","warmup_ratio = 0.1\n","max_grad_norm = 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_9uz_2PQQGZx"},"outputs":[],"source":["#train \u0026 val 데이터로 나누기\n","from sklearn.model_selection import train_test_split\n","\n","df_train, df_val = train_test_split(data_list, test_size=0.2, shuffle=True, random_state=34)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ahp9d8WkJs2U"},"outputs":[],"source":["df_train = [[str(text), int(label)] for text, label in df_train]\n","df_val = [[str(text), int(label)] for text, label in df_val]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJ7JC-ZtR5V2"},"outputs":[],"source":["tok=tokenizer.tokenize\n","data_train = BERTDataset(df_train, 0, 1, tok, vocab, max_len, True, False)\n","data_val = BERTDataset(df_val,0, 1, tok, vocab,  max_len, True, False)"]},{"cell_type":"markdown","metadata":{"id":"hN4ypJ6UPcIm"},"source":["**dataloader**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gfHmmu8rOsXC"},"outputs":[],"source":["from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","\n","def ret_dataloader(batch_size):\n","    train_dataloader = DataLoader(\n","                data_train,  # The training samples.\n","                sampler = RandomSampler(data_train), # Select batches randomly\n","                batch_size = batch_size # Trains with this batch size.\n","            )\n","\n","    validation_dataloader = DataLoader(\n","                data_val, # The validation samples.\n","                sampler = SequentialSampler(data_val), # Pull out batches sequentially.\n","                batch_size = batch_size # Evaluate with this batch size.\n","            )\n","    return train_dataloader,validation_dataloader"]},{"cell_type":"markdown","metadata":{"id":"eC9TLATLRSJ_"},"source":["**Bert Classifier**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qWnsvhpxR5Ot"},"outputs":[],"source":["class BERTClassifier(nn.Module):\n","    def __init__(self,\n","                 bert, # bertmodel 입력받음.\n","                 dr_rate,\n","                 hidden_size = 768,\n","                 num_classes=2,   ##클래스 수 조정##\n","                 params=None):\n","        super(BERTClassifier, self).__init__()\n","        self.bert = bert\n","        self.dr_rate = dr_rate\n","\n","        self.classifier = nn.Linear(hidden_size , num_classes)\n","        if dr_rate:\n","            self.dropout = nn.Dropout(p=dr_rate)\n","\n","    def gen_attention_mask(self, token_ids, valid_length):\n","        attention_mask = torch.zeros_like(token_ids)\n","        for i, v in enumerate(valid_length):\n","            attention_mask[i][:v] = 1\n","        return attention_mask.float()\n","\n","    def forward(self, token_ids, valid_length, segment_ids):\n","        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n","\n","        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(),\n","                              attention_mask = attention_mask.float().to(token_ids.device),return_dict=False)\n","        if self.dr_rate:\n","            out = self.dropout(pooler)\n","        return self.classifier(out)"]},{"cell_type":"markdown","metadata":{"id":"POFnjpYrQqJ3"},"source":["**optimizer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bIwEI0DOQh5D"},"outputs":[],"source":["def ret_optim(model, lr_rate):\n","    optimizer = AdamW(model.parameters(),\n","                      lr = lr_rate,\n","                      eps = 1e-8\n","                    )\n","    return optimizer"]},{"cell_type":"markdown","metadata":{"id":"T4v2bQ15QsI4"},"source":["**scheduler**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qNzmSs3qQloc"},"outputs":[],"source":["def ret_scheduler(train_dataloader, optimizer, epochs):\n","    # Total number of training steps is [number of batches] x [number of epochs].\n","    # (Note that this is not the same as the number of training samples).\n","    total_steps = len(train_dataloader) * epochs\n","    warmup_step = int(total_steps * warmup_ratio)\n","\n","    # Create the learning rate scheduler.\n","    scheduler = get_cosine_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps = warmup_step,\n","                                                num_training_steps = total_steps)\n","    return scheduler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7nvzNcYLLTu5"},"outputs":[],"source":["#정확도 측정을 위한 함수 정의\n","def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CGetAEC2UL5p"},"outputs":[],"source":["import time\n","import datetime\n","\n","def format_time(elapsed):\n","    '''\n","    Takes a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second.\n","    elapsed_rounded = int(round((elapsed)))\n","\n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds=elapsed_rounded))"]},{"cell_type":"markdown","metadata":{"id":"hZKZakpnSPMK"},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"94CqUUA8LTtA"},"outputs":[],"source":["def train(epochs, lr_rate, batch_size, dr_rate):\n","    # model initialize\n","    model = BERTClassifier(bertmodel, dr_rate=dr_rate)\n","    model.to(device)\n","\n","    train_dataloader,validation_dataloader = ret_dataloader(batch_size)\n","    optimizer = ret_optim(model, lr_rate)\n","    scheduler = ret_scheduler(train_dataloader, optimizer, epochs)\n","    loss_fn = nn.CrossEntropyLoss()\n","\n","    # We'll store a number of quantities such as training and validation loss,\n","    # validation accuracy, and timings.\n","    training_stats = []\n","\n","    # Measure the total training time for the whole run.\n","    total_t0 = time.time()\n","\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        # Measure how long the training epoch takes.\n","        t0 = time.time()\n","\n","        # Reset the total loss for this epoch.\n","        total_train_loss = 0\n","\n","        model.train()\n","\n","        # For each batch of training data...\n","        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n","            optimizer.zero_grad()\n","\n","            token_ids = token_ids.long().to(device)\n","            segment_ids = segment_ids.long().to(device)\n","            valid_length= valid_length\n","            label = label.long().to(device)\n","\n","            out = model(token_ids, valid_length, segment_ids)\n","\n","            loss = loss_fn(out, label)\n","\n","            # Accumulate the training loss over all of the batches so that we can\n","            # calculate the average loss at the end. `loss` is a Tensor containing a\n","            # single value; the `.item()` function just returns the Python value\n","            # from the tensor.\n","            total_train_loss += loss.item()\n","\n","            # Perform a backward pass to calculate the gradients.\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            optimizer.step()\n","\n","            # Update the learning rate.\n","            scheduler.step()\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure our performance on\n","        # our validation set.\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation.\n","        model.eval()\n","\n","        # Tracking variables\n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(validation_dataloader):\n","            token_ids = token_ids.long().to(device)\n","            segment_ids = segment_ids.long().to(device)\n","            valid_length= valid_length\n","            label = label.long().to(device)\n","\n","            with torch.no_grad() :\n","                out = model(token_ids, valid_length, segment_ids)\n","\n","                loss = loss_fn(out, label)\n","\n","            # Accumulate the validation loss.\n","            total_eval_loss += loss.item()\n","\n","            # Calculate the accuracy for this batch of test sentences, and\n","            # accumulate it over all batches.\n","            total_eval_accuracy += calc_accuracy(out, label)\n","\n","        # Report the final accuracy for this validation run.\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","        # Measure how long the validation run took.\n","        validation_time = format_time(time.time() - t0)\n","\n","        print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","        # Record all statistics from this epoch.\n","        training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time\n","            }\n","        )\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":890645,"status":"ok","timestamp":1724044382565,"user":{"displayName":"‎주민서(인공지능대학 인공지능학과)","userId":"08406903885902536930"},"user_tz":-540},"id":"y-N2fQzFNfge","outputId":"798b1ba5-fbd7-4d5b-b501-c4839dd01589"},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","======== Epoch 1 / 5 ========\n","Training...\n","\n","  Average training loss: 0.4993\n","  Training epcoh took: 0:02:44\n","\n","Running Validation...\n","  Accuracy: 0.8667\n","  Validation Loss: 0.3816\n","  Validation took: 0:00:12\n","\n","======== Epoch 2 / 5 ========\n","Training...\n","\n","  Average training loss: 0.3330\n","  Training epcoh took: 0:02:46\n","\n","Running Validation...\n","  Accuracy: 0.8882\n","  Validation Loss: 0.3231\n","  Validation took: 0:00:12\n","\n","======== Epoch 3 / 5 ========\n","Training...\n","\n","  Average training loss: 0.2703\n","  Training epcoh took: 0:02:46\n","\n","Running Validation...\n","  Accuracy: 0.8898\n","  Validation Loss: 0.3680\n","  Validation took: 0:00:12\n","\n","======== Epoch 4 / 5 ========\n","Training...\n","\n","  Average training loss: 0.2183\n","  Training epcoh took: 0:02:46\n","\n","Running Validation...\n","  Accuracy: 0.9089\n","  Validation Loss: 0.3296\n","  Validation took: 0:00:12\n","\n","======== Epoch 5 / 5 ========\n","Training...\n","\n","  Average training loss: 0.1866\n","  Training epcoh took: 0:02:46\n","\n","Running Validation...\n","  Accuracy: 0.9094\n","  Validation Loss: 0.3561\n","  Validation took: 0:00:12\n","\n","Training complete!\n","Total training took 0:14:50 (h:mm:ss)\n"]}],"source":["model = train(epochs=5, lr_rate=2e-5, batch_size=8, dr_rate=0.3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aEeGHcK7PeMc"},"outputs":[],"source":["# save model\n","\n","torch.save(model.state_dict(), '/content/drive/MyDrive/유런 24 여름 방학 프로젝트/BERT/KoBERT/kobert_plain.pth')"]},{"cell_type":"markdown","metadata":{"id":"g7LGaVUaOgNH"},"source":["**llrd 적용**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tPGQJSjgOjt9"},"outputs":[],"source":["def ret_optim_llrd(model, lr_rate):\n","    # LLRD 적용\n","    optimizer_grouped_parameters = [\n","        {'params': model.bert.encoder.layer[:6].parameters(), 'lr': lr_rate * 0.1},\n","        {'params': model.bert.encoder.layer[6:12].parameters(), 'lr': lr_rate * 0.5},\n","        {'params': model.classifier.parameters(), 'lr': lr_rate}\n","    ]\n","\n","    optimizer = AdamW(optimizer_grouped_parameters, eps=1e-8)\n","    return optimizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mAQONLjvQYq6"},"outputs":[],"source":["def train(epochs, lr_rate, batch_size, dr_rate):\n","\n","    model = BERTClassifier(bertmodel, dr_rate=dr_rate)\n","    model.to(device)\n","\n","    train_dataloader,validation_dataloader = ret_dataloader(batch_size)\n","    optimizer = ret_optim_llrd(model, lr_rate) #llrd_optimizer 적용\n","    scheduler = ret_scheduler(train_dataloader, optimizer, epochs)\n","    loss_fn = nn.CrossEntropyLoss()\n","\n","    # We'll store a number of quantities such as training and validation loss,\n","    # validation accuracy, and timings.\n","    training_stats = []\n","\n","    # Measure the total training time for the whole run.\n","    total_t0 = time.time()\n","\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        # Measure how long the training epoch takes.\n","        t0 = time.time()\n","\n","        # Reset the total loss for this epoch.\n","        total_train_loss = 0\n","\n","        model.train()\n","\n","        # For each batch of training data...\n","        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(train_dataloader):\n","            optimizer.zero_grad()\n","\n","            token_ids = token_ids.long().to(device)\n","            segment_ids = segment_ids.long().to(device)\n","            valid_length= valid_length\n","            label = label.long().to(device)\n","\n","            out = model(token_ids, valid_length, segment_ids)\n","\n","            loss = loss_fn(out, label)\n","\n","            # Accumulate the training loss over all of the batches so that we can\n","            # calculate the average loss at the end. `loss` is a Tensor containing a\n","            # single value; the `.item()` function just returns the Python value\n","            # from the tensor.\n","            total_train_loss += loss.item()\n","\n","            # Perform a backward pass to calculate the gradients.\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            optimizer.step()\n","\n","            # Update the learning rate.\n","            scheduler.step()\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)\n","\n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure our performance on\n","        # our validation set.\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation.\n","        model.eval()\n","\n","        # Tracking variables\n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","        for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(validation_dataloader):\n","            token_ids = token_ids.long().to(device)\n","            segment_ids = segment_ids.long().to(device)\n","            valid_length= valid_length\n","            label = label.long().to(device)\n","\n","            with torch.no_grad() :\n","                out = model(token_ids, valid_length, segment_ids)\n","\n","                loss = loss_fn(out, label)\n","\n","            # Accumulate the validation loss.\n","            total_eval_loss += loss.item()\n","\n","            # Calculate the accuracy for this batch of test sentences, and\n","            # accumulate it over all batches.\n","            total_eval_accuracy += calc_accuracy(out, label)\n","\n","        # Report the final accuracy for this validation run.\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","\n","        # Measure how long the validation run took.\n","        validation_time = format_time(time.time() - t0)\n","\n","        print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","        # Record all statistics from this epoch.\n","        training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time\n","            }\n","        )\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","    return model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":529304,"status":"ok","timestamp":1723814909745,"user":{"displayName":"문가을","userId":"07477440285791126970"},"user_tz":-540},"id":"wimA0avpQDzI","outputId":"a524900b-4b2d-4ddd-e7f2-d17bb34e9870"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","======== Epoch 1 / 3 ========\n","Training...\n","\n","  Average training loss: 0.2221\n","  Training epcoh took: 0:02:44\n","\n","Running Validation...\n","  Accuracy: 0.9036\n","  Validation Loss: 0.4186\n","  Validation took: 0:00:12\n","\n","======== Epoch 2 / 3 ========\n","Training...\n","\n","  Average training loss: 0.1622\n","  Training epcoh took: 0:02:44\n","\n","Running Validation...\n","  Accuracy: 0.9089\n","  Validation Loss: 0.3760\n","  Validation took: 0:00:12\n","\n","======== Epoch 3 / 3 ========\n","Training...\n","\n","  Average training loss: 0.1334\n","  Training epcoh took: 0:02:45\n","\n","Running Validation...\n","  Accuracy: 0.9068\n","  Validation Loss: 0.4107\n","  Validation took: 0:00:12\n","\n","Training complete!\n","Total training took 0:08:49 (h:mm:ss)\n"]}],"source":["model = train(epochs=3, lr_rate=5e-5, batch_size=8, dr_rate=0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JzCyMFgCQdt0"},"outputs":[],"source":["# save model\n","\n","torch.save(model.state_dict(), '/content/drive/MyDrive/유런 24 여름 방학 프로젝트/BERT/KoBERT/kobert_llrd.pth')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DzXPjEr6R60r"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
