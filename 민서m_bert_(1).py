# -*- coding: utf-8 -*-
"""민서M-BERT (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gHV3w5MRLJ8CIorDOe-yrNoxSiaNWARr

## 패키지 import
"""

# 한글 폰트 설치
!sudo apt-get install -y fonts-nanum
!sudo fc-cache -fv
!rm ~/.cache/matplotlib -rf

# 폰트 설치 확인
import  matplotlib
import  matplotlib.font_manager  as fm
import  matplotlib.pyplot  as plt


sys_font  = fm.findSystemFonts ( )

[ font  for  font  in  sys_font  if  "Nanum"  in font ]

# 나눔 폰트 설정
font_path = "/usr/share/fonts/truetype/nanum/NanumGothicBold.ttf"

font_name  = fm.FontProperties(fname=font_path, size=12).get_name( )

print("◎ 폰트 이름 : ",font_name)

plt.rc("font", family= font_name)

# 드라이브 마운트
from google.colab import drive
drive.mount('/content/drive')

!pip install konlpy --q

!pip install nltk --q

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from collections import Counter
from wordcloud import WordCloud
import re

import tensorflow as tf
import torch
from torch.optim import AdamW, lr_scheduler

from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from nltk.tag import pos_tag
import nltk
from konlpy.tag import Komoran
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('wordnet')

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from transformers import BertTokenizer, BertModel, BertForSequenceClassification, BertConfig

"""# M-BERT"""

file_path = '/content/drive/MyDrive/유런 24 여름 방학 프로젝트/eda&전처리/preprocessing_final.csv'
final_df = pd.read_csv(file_path)

last_df = pd.DataFrame()
last_df['title'] = final_df['processed_title']
last_df['label'] = final_df['label']

last_df

# train/eval split

texts = last_df['title']
labels = last_df['label']

X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size = 0.2, random_state = 42)

# index initialization
X_train = X_train.reset_index(drop = True)
X_test = X_test.reset_index(drop = True)
y_train = y_train.reset_index(drop = True)
y_test = y_test.reset_index(drop = True)

X_train_fixed = ['[CLS]' + str(text) + '[SEP]' for text in X_train]
X_test_fixed = ['[CLS]' + str(text) + '[SEP]' for text in X_test]

print(X_train_fixed)

# tokenizing using multilingual

tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case = False)
tokenized_X_train = [tokenizer.tokenize(fixed) for fixed in X_train_fixed]
tokenized_X_test = [tokenizer.tokenize(fixed) for fixed in X_test_fixed]

print(tokenized_X_train[0])

X_train_ids = [tokenizer.convert_tokens_to_ids(fixed) for fixed in tokenized_X_train]
X_test_ids = [tokenizer.convert_tokens_to_ids(fixed) for fixed in tokenized_X_test]

print(X_train_ids[0]) # 토크나이징 된 단어를 id로 나타냄

# 최대 길이가 128로 설정, 나머지 빈 부분은 0으로 패딩해줌
max_len = 128
# pad_sequences: 시퀀스 데이터를 일정한 길이(max_len)로 맞추는 데 사용

X_train_ids = pad_sequences(X_train_ids, maxlen = max_len, dtype = 'long', padding = 'post', truncating = 'post')
X_test_ids = pad_sequences(X_test_ids, maxlen = max_len, dtype = 'long', padding = 'post', truncating = 'post')

print(X_train_ids[0])

# masking
mask_train = []
attention_mask_train = []

for id in X_train_ids: # X_train_ids는 이미 토큰화 + 임베딩 + 패딩까지 완료된 상태, id는 한 문장에 들어있는 토큰들의 id list
  mask_train = [float(i>0) for i in id] # 패딩 토큰(0)에는 0.0, 실제 토큰에는 1.0으로 채워진 마스크 생성함으로써 패딩 토큰은 고려하지 않게 됨
  attention_mask_train.append(mask_train) # 각 id list마다 mask가 만들어짐

mask_test = []
attention_mask_test = []

for id in X_test_ids:
  mask_test = [float(i>0) for i in id]
  attention_mask_test.append(mask_test)

# 텐서로 바꿔줌

X_train_ids = torch.tensor(X_train_ids)
attention_mask_train = torch.tensor(attention_mask_train)
y_train = torch.tensor(y_train)

X_test_ids = torch.tensor(X_test_ids)
attention_mask_test = torch.tensor(attention_mask_test)
y_test = torch.tensor(y_test)

# hyperparameter setting

BATCH_SIZE = 32
EPOCHS = 10

STEP_SIZE = 3
GAMMA = 0.1

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# train setting

train_data = TensorDataset(X_train_ids, attention_mask_train, y_train) # 같은 위치에 있는 데이터를 튜플 형태로 묶어줌 ex. (X_train_ids[0], attention_mask_train[0], y_train[0])
train_sampler = RandomSampler(train_data)
train_dataloader = DataLoader(train_data, batch_size = BATCH_SIZE, sampler = train_sampler)

test_data = TensorDataset(X_test_ids, attention_mask_test, y_test)
test_dataloader = DataLoader(test_data, batch_size = BATCH_SIZE)

model = BertForSequenceClassification.from_pretrained("bert-base-multilingual-cased", num_labels = 2).to(DEVICE)
optimizer = AdamW(model.parameters(), lr = 3e-5, eps = 1e-8)
lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = STEP_SIZE, gamma = GAMMA)

print(f"length of X_train_ids: {len(X_train_ids)}")
print(f"length of train_dataloader: {len(train_dataloader)}") # = len(X_train_ids) // BATCH_SIZE

print(f"학습율: {optimizer.param_groups[0]['lr']}")

# training

train_loss_list = []
train_acc_list = []
eval_loss_list = []
eval_acc_list = []


best_loss = 10**9
patience_limit = 3
patience_check = 0


for epoch in range(EPOCHS):

  print(f"=====EPOCH {epoch+1} / {EPOCHS}======")
  print(f"학습율: {optimizer.param_groups[0]['lr']}") # 학습율이 잘 업데이트되는지 확인

  total_loss = 0
  train_acc = []
  model.train()

  for idx, batch in enumerate(train_dataloader):

    batch = tuple(t.to(DEVICE) for t in batch) # batch에는 (X_train_ids, attention_mask_train, y_train)이 들어있는데, 각각의 요소를 뽑아서 cuda로 보내고 새로운 튜플을 만듦

    input_ids, input_mask, labels = batch

    # forward pass
    outputs = model(input_ids, token_type_ids = None, attention_mask = input_mask, labels = labels)
    loss = outputs[0]
    total_loss += loss.item()

    pred = np.array([torch.argmax(logit).cpu().detach().item() for logit in outputs.logits])
    true = np.array([label for label in labels.cpu().numpy()])

    accuracy = np.sum(pred == true) / true.shape[0]
    train_acc.append(accuracy)

    loss.backward() # backpropagation으로 기울기 계산

    optimizer.step() # 최적화 함수로 파라미터 업데이트

    model.zero_grad() # 모델 파라미터의 기울기 초기화

  avg_train_loss = total_loss / len(train_dataloader) # 에폭 한 번 당 평균 loss를 구함
  train_loss_list.append(avg_train_loss)
  avg_train_acc = np.mean(train_acc)
  train_acc_list.append(avg_train_acc)

  lr_scheduler.step()

  eval_loss = 0
  eval_acc = []
  model.eval()

  criterion = torch.nn.CrossEntropyLoss()

  for batch in test_dataloader:

    batch = tuple(t.to(DEVICE) for t in batch)
    input_ids, input_mask, labels = batch

    with torch.no_grad(): # 기울기 계산 비활성화
      # forward pass
      outputs = model(input_ids, token_type_ids = None, attention_mask = input_mask)
      loss = criterion(outputs.logits, labels)
      eval_loss += loss.item()
    pred = np.array([torch.argmax(logit).cpu().detach().item() for logit in outputs.logits])
    true = np.array([label for label in labels.cpu().numpy()])
    accuracy = np.sum(pred == true) / true.shape[0]
    eval_acc.append(accuracy)

  if eval_loss > best_loss: # loss가 개선되지 않은 경우
    patience_check += 1

    if patience_check >= patience_limit: # early stopping 조건 만족(즉, eval loss가 3번 동안이나 개선되지 않는 경우)
      break

  else: # eval loss가 개선되는 경우
    best_loss = eval_loss
    patience_check = 0


  avg_eval_loss = eval_loss / len(test_dataloader)
  eval_loss_list.append(avg_eval_loss)
  avg_eval_acc = np.mean(eval_acc)
  eval_acc_list.append(avg_eval_acc)

  print("-----train-----")
  print(f"training loss: {avg_train_loss:.5f}")
  print(f"training acc: {avg_train_acc:.5f}")

  print("-----eval-----")
  print(f"eval loss: {avg_eval_loss:.5f}")
  print(f"eval acc: \033[91m{avg_eval_acc:.5f}\033[0m")

# save model
# torch.save(model.state_dict(), '/content/drive/MyDrive/m-bert_ES.pth')

"""# Visualization"""

import matplotlib.pyplot as plt

fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (12, 5))

ax1.plot(range(1, 8), train_loss_list, label = 'train loss', color = 'blue')
ax1.legend()
ax1.set_ylabel('train loss')
ax1.set_xlabel('epoch')

ax2.plot(range(1, 7), eval_loss_list, label = 'eval loss', color = 'red')
ax2.legend()
ax2.set_ylabel('eval loss')
ax2.set_xlabel('epoch')

fig, (ax3, ax4) = plt.subplots(1, 2, figsize = (12, 5))

ax3.plot(range(1, 8), train_acc_list, label = 'train acc', color = 'blue')
ax3.legend()
ax3.set_ylabel('train acc')
ax3.set_xlabel('epoch')

ax4.plot(range(1, 7), eval_acc_list, label = 'eval acc', color = 'red')
ax4.legend()
ax4.set_ylabel('eval acc')
ax4.set_xlabel('epoch')

plt.tight_layout()
plt.show()

# heatmap

preds, trues = list(), list()

with torch.no_grad():
  for idx, batch in enumerate(test_dataloader):
    batch = tuple(t.to(DEVICE) for t in batch)
    input_ids, input_mask, labels = batch
    y_pred = model(input_ids, token_type_ids = None, attention_mask = input_mask)
    preds.extend([torch.argmax(predicted).cpu().detach().item() for predicted in y_pred.logits])
    trues.extend([label.cpu().item() for label in labels])

test_df = pd.DataFrame({'pred': preds, 'label': trues})
test_df

import seaborn as sns

confusion_matrix = pd.crosstab(test_df['label'], test_df['pred'])
confusion_matrix.index = test_df['pred'].unique()
confusion_matrix.columns = test_df['label'].unique()
confusion_matrix = confusion_matrix.iloc[::-1, :]

sns.heatmap(confusion_matrix, cmap = 'Purples', annot = True, linewidth = .5, fmt = ".0f")
plt.title("Heatmap")
plt.ylabel("label")
plt.xlabel("pred")
plt.show()